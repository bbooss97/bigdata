{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "I used those commands to connect to my google drive in order to save and retrieve the preprocessed data without starting from scratch\n"
      ],
      "metadata": {
        "id": "koJohF3Szoya"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnYsCCKUC0KI",
        "outputId": "fd948669-08e5-45ec-ce1c-78f2cd3808d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ManCNBMLDlH1",
        "outputId": "b1c452b5-5d83-445d-8489-b781268c9e25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/MyDrive/bigdata\n"
          ]
        }
      ],
      "source": [
        "%cd ../gdrive/MyDrive/bigdata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1onsDZrC_hn",
        "outputId": "41d5927b-df31-4463-b4df-8e32a472e104"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "amazon-fine-food-reviews.zip  hashes.txt   Reviews.csv\tX.pkl\n",
            "database.sqlite\t\t      kaggle.json  reviews.txt\n"
          ]
        }
      ],
      "source": [
        "!ls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFpFKOVg-saJ",
        "outputId": "e577d157-6d4f-45fb-ff48-b46e82d9c1fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pickle-mixin\n",
            "  Downloading pickle-mixin-1.0.2.tar.gz (5.1 kB)\n",
            "Building wheels for collected packages: pickle-mixin\n",
            "  Building wheel for pickle-mixin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pickle-mixin: filename=pickle_mixin-1.0.2-py3-none-any.whl size=6008 sha256=0167e1fea567cd79354c0e4a7f516283cad58113491f6f41cf54b3588df25976\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/70/0b/673e09a7ed429660d22352a1b117b4f616a8fc054bdd7eb157\n",
            "Successfully built pickle-mixin\n",
            "Installing collected packages: pickle-mixin\n",
            "Successfully installed pickle-mixin-1.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pickle-mixin --user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mxLnM9Jf8p27"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import random \n",
        "import numpy as np\n",
        "import pickle\n",
        "import scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "used this to insert my kaggle key to download the dataset from kaggle without uploading it"
      ],
      "metadata": {
        "id": "EOQ4jO040BHF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "4yUHnIny9qgH",
        "outputId": "b8ec9156-29a4-4b0c-a025-db4f17d145e2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-45e0c5ae-7d03-4cac-8988-e7352feeebd6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-45e0c5ae-7d03-4cac-8988-e7352feeebd6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Umah44d798wk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPPRCuE7-PkN"
      },
      "outputs": [],
      "source": [
        "!chmod 600 /content/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-15ztg1-Ag3"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d snap/amazon-fine-food-reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4hipK86-IKc"
      },
      "outputs": [],
      "source": [
        "from zipfile import ZipFile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIWJMzFd_YcW"
      },
      "outputs": [],
      "source": [
        "# Create a ZipFile Object and load sample.zip in it\n",
        "with ZipFile('amazon-fine-food-reviews.zip', 'r') as zipObj:\n",
        "   # Extract all the contents of zip file in current directory\n",
        "   zipObj.extractall()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "i used the preprocessed procedure linked in the classroom. I basically remove stop words and i transform everything into tfidf vectors."
      ],
      "metadata": {
        "id": "_JNbnXZN0ORt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZM7yilBK8p3B"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.tokenize import word_tokenize #Used to extract words from documents\n",
        "from nltk.stem import WordNetLemmatizer #Used to lemmatize words\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "read dataset and shuffle it so that it is easier to use just a subset of it to do testings."
      ],
      "metadata": {
        "id": "7ZBG4Di40fzQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKorWFxQ8p3D"
      },
      "outputs": [],
      "source": [
        "#read data from csv and get the reviews\n",
        "df = pd.read_csv('Reviews.csv')\n",
        "reviews=df[\"Text\"].tolist()\n",
        "#shuffle to later divide in test and training without bias\n",
        "random.shuffle(reviews)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "remove contraddictions and abbreviations to simplify the analysis of documents"
      ],
      "metadata": {
        "id": "beQnB0g001wu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNio9OcW8p3E"
      },
      "outputs": [],
      "source": [
        "#preprocess the reviews\n",
        "contractions_dict = {\n",
        "  \"ain't\": \"am not\",\n",
        "  \"aren't\": \"are not\",\n",
        "  \"can't\": \"cannot\",\n",
        "  \"can't've\": \"cannot have\",\n",
        "  \"'cause\": \"because\",\n",
        "  \"could've\": \"could have\",\n",
        "  \"couldn't\": \"could not\",\n",
        "  \"couldn't've\": \"could not have\",\n",
        "  \"didn't\": \"did not\",\n",
        "  \"doesn't\": \"does not\",\n",
        "  \"don't\": \"do not\",\n",
        "  \"hadn't\": \"had not\",\n",
        "  \"hadn't've\": \"had not have\",\n",
        "  \"hasn't\": \"has not\",\n",
        "  \"haven't\": \"have not\",\n",
        "  \"he'd\": \"he would\",\n",
        "  \"he'd've\": \"he would have\",\n",
        "  \"he'll\": \"he will\",\n",
        "  \"he'll've\": \"he will have\",\n",
        "  \"he's\": \"he is\",\n",
        "  \"how'd\": \"how did\",\n",
        "  \"how'd'y\": \"how do you\",\n",
        "  \"how'll\": \"how will\",\n",
        "  \"how's\": \"how is\",\n",
        "  \"i'd\": \"I would\",\n",
        "  \"i'd've\": \"I would have\",\n",
        "  \"i'll\": \"I will\",\n",
        "  \"i'll've\": \"I will have\",\n",
        "  \"i'm\": \"I am\",\n",
        "  \"i've\": \"I have\",\n",
        "  \"isn't\": \"is not\",\n",
        "  \"it'd\": \"it had\",\n",
        "  \"it'd've\": \"it would have\",\n",
        "  \"it'll\": \"it will\",\n",
        "  \"it'll've\": \"it will have\",\n",
        "  \"it's\": \"it is\",\n",
        "  \"let's\": \"let us\",\n",
        "  \"ma'am\": \"madam\",\n",
        "  \"mayn't\": \"may not\",\n",
        "  \"might've\": \"might have\",\n",
        "  \"mightn't\": \"might not\",\n",
        "  \"mightn't've\": \"might not have\",\n",
        "  \"must've\": \"must have\",\n",
        "  \"mustn't\": \"must not\",\n",
        "  \"mustn't've\": \"must not have\",\n",
        "  \"needn't\": \"need not\",\n",
        "  \"needn't've\": \"need not have\",\n",
        "  \"o'clock\": \"of the clock\",\n",
        "  \"oughtn't\": \"ought not\",\n",
        "  \"oughtn't've\": \"ought not have\",\n",
        "  \"shan't\": \"shall not\",\n",
        "  \"sha'n't\": \"shall not\",\n",
        "  \"shan't've\": \"shall not have\",\n",
        "  \"she'd\": \"she would\",\n",
        "  \"she'd've\": \"she would have\",\n",
        "  \"she'll\": \"she will\",\n",
        "  \"she'll've\": \"she will have\",\n",
        "  \"she's\": \"she is\",\n",
        "  \"should've\": \"should have\",\n",
        "  \"shouldn't\": \"should not\",\n",
        "  \"shouldn't've\": \"should not have\",\n",
        "  \"so've\": \"so have\",\n",
        "  \"so's\": \"so is\",\n",
        "  \"that'd\": \"that would\",\n",
        "  \"that'd've\": \"that would have\",\n",
        "  \"that's\": \"that is\",\n",
        "  \"there'd\": \"there had\",\n",
        "  \"there'd've\": \"there would have\",\n",
        "  \"there's\": \"there is\",\n",
        "  \"they'd\": \"they would\",\n",
        "  \"they'd've\": \"they would have\",\n",
        "  \"they'll\": \"they will\",\n",
        "  \"they'll've\": \"they will have\",\n",
        "  \"they're\": \"they are\",\n",
        "  \"they've\": \"they have\",\n",
        "  \"to've\": \"to have\",\n",
        "  \"wasn't\": \"was not\",\n",
        "  \"we'd\": \"we had\",\n",
        "  \"we'd've\": \"we would have\",\n",
        "  \"we'll\": \"we will\",\n",
        "  \"we'll've\": \"we will have\",\n",
        "  \"we're\": \"we are\",\n",
        "  \"we've\": \"we have\",\n",
        "  \"weren't\": \"were not\",\n",
        "  \"what'll\": \"what will\",\n",
        "  \"what'll've\": \"what will have\",\n",
        "  \"what're\": \"what are\",\n",
        "  \"what's\": \"what is\",\n",
        "  \"what've\": \"what have\",\n",
        "  \"when's\": \"when is\",\n",
        "  \"when've\": \"when have\",\n",
        "  \"where'd\": \"where did\",\n",
        "  \"where's\": \"where is\",\n",
        "  \"where've\": \"where have\",\n",
        "  \"who'll\": \"who will\",\n",
        "  \"who'll've\": \"who will have\",\n",
        "  \"who's\": \"who is\",\n",
        "  \"who've\": \"who have\",\n",
        "  \"why's\": \"why is\",\n",
        "  \"why've\": \"why have\",\n",
        "  \"will've\": \"will have\",\n",
        "  \"won't\": \"will not\",\n",
        "  \"won't've\": \"will not have\",\n",
        "  \"would've\": \"would have\",\n",
        "  \"wouldn't\": \"would not\",\n",
        "  \"wouldn't've\": \"would not have\",\n",
        "  \"y'all\": \"you all\",\n",
        "  \"y'alls\": \"you alls\",\n",
        "  \"y'all'd\": \"you all would\",\n",
        "  \"y'all'd've\": \"you all would have\",\n",
        "  \"y'all're\": \"you all are\",\n",
        "  \"y'all've\": \"you all have\",\n",
        "  \"you'd\": \"you had\",\n",
        "  \"you'd've\": \"you would have\",\n",
        "  \"you'll\": \"you will\",\n",
        "  \"you'll've\": \"you will have\",\n",
        "  \"you're\": \"you are\",\n",
        "  \"you've\": \"you have\"\n",
        "}\n",
        "\n",
        "# Regular expression for finding contractions\n",
        "def multiple_replace(dict, text):\n",
        "  # Create a regular expression  from the dictionary keys\n",
        "  regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
        "\n",
        "  # For each match, look-up corresponding value in dictionary\n",
        "  return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text)\n",
        "\n",
        "reviews = [multiple_replace(contractions_dict, doc) for doc in reviews]\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "reviews = [doc.translate(table) for doc in reviews]\n",
        "reviews = [re.sub(r'\\d+', \" \", doc) for doc in reviews]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lemmatize the document in order to group words that mean the same thing and so on to have an analysis on the semantic"
      ],
      "metadata": {
        "id": "8f7mjq_o1C5j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhBBLErx8p3G"
      },
      "outputs": [],
      "source": [
        "def get_lemmatized(doc):\n",
        "  word_list = word_tokenize(doc)\n",
        "  lemmatized_doc = \"\"\n",
        "  for word in word_list:\n",
        "    lemmatized_doc = lemmatized_doc + \" \" + lemmatizer.lemmatize(word)\n",
        "  return lemmatized_doc\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "reviews = [get_lemmatized(doc) for doc in reviews]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "remove stopwords that don't capture things that are useful"
      ],
      "metadata": {
        "id": "L2IXNnGU1YZM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdaBWCJF8p3H"
      },
      "outputs": [],
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english') # Returns a list\n",
        "stopwords = set(stopwords) # We want a set, because this is implemented with a hash table\n",
        "                           # Checking the if condition in rem_stop costs O(1) in this way\n",
        "\n",
        "def rem_stop(doc):\n",
        "   word_list = word_tokenize(doc)\n",
        "   cleaned_doc = \"\"\n",
        "   for word in word_list:\n",
        "     if word not in stopwords:\n",
        "       cleaned_doc += \" \" + word\n",
        "   return cleaned_doc\n",
        "\n",
        "\n",
        "reviews = [rem_stop(doc) for doc in reviews]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "vectorize the documents with tfidf to capture informations based on the frequencies of the words in the document.Not frequently used words are more useful."
      ],
      "metadata": {
        "id": "2CzcjA521iIH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpDDu1lr8p3I"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(stop_words='english', min_df=10) ## Corpus is in English\n",
        "X = vectorizer.fit_transform(reviews)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "used this to save reviews without redoing everything"
      ],
      "metadata": {
        "id": "rInPpYuf11gg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3pFcXKq8p3J"
      },
      "outputs": [],
      "source": [
        "# #write reviews to file\n",
        "with open('reviews.txt', 'w',encoding='utf8') as f:\n",
        "    for item in reviews:\n",
        "        f.write(item+\",\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "save the processed document on the drive to reload it without redo everything everytime i disconnect the environment"
      ],
      "metadata": {
        "id": "RU23ihv-2JpL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiUUpBaf8p3J"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "#write x to file\n",
        "with open('X.pkl', 'wb') as f:\n",
        "\tpickle.dump(X, f)\n",
        "\n",
        "# read obj from file\n",
        "with open('X.pkl', 'rb') as f:\n",
        "\tobj = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this function reads the preprocessed documents saved previously "
      ],
      "metadata": {
        "id": "SWS-v48_2UPP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Tn7dSd-P8p3L"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "def readReviewsX():\n",
        "  #read reviews from file\n",
        "  with open('reviews.txt', 'r',encoding='utf8') as f:\n",
        "      reviews = f.read().split(\",\")\n",
        "      reviews = reviews[:-1]\n",
        "  #read X from file\n",
        "  with open('X.pkl', 'rb') as f:\n",
        "      X = pickle.load(f)\n",
        "  return reviews,X\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "take just a percentage of the dataset to test everything on a small subset"
      ],
      "metadata": {
        "id": "ceaD7OiO8e_7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "a_3goAj88p3L"
      },
      "outputs": [],
      "source": [
        "def percentageOfDataset(reviews,X,percentage=1):\n",
        "  #take just a percentage of the dataset\n",
        "  reviewsOut=reviews[:int(len(reviews)*percentage)]\n",
        "  XOut=X[:int(X.shape[0]*percentage)]\n",
        "  return reviewsOut,XOut\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this class contains some operations that are useful both for the normal lsh and for the lsh vectorized"
      ],
      "metadata": {
        "id": "ZWEqjE5S25Bp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "X3P3-1p18p3M"
      },
      "outputs": [],
      "source": [
        "class Operations: \n",
        "    #generate a random vector direction used to calculate the hashes saved in the matrix m\n",
        "    def generateU():\n",
        "        u=np.random.normal(0,1,cols)\n",
        "        u=scipy.sparse.csr_matrix(u)\n",
        "        return u\n",
        "    #sparse dot product for sparse sci py matrices\n",
        "    def sparseProduct(x,y):\n",
        "        return x.dot(y.transpose()).toarray()[0][0]\n",
        "    #given u and a vector it returns the hash to be saved in the matrix m\n",
        "    def h(x,u):\n",
        "        return np.sign(Operations.sparseProduct(x,u))\n",
        "    #returns the exact cosine similarity of 2 vectors\n",
        "    def cosineSimilarity(x,y):\n",
        "        r=Operations.sparseProduct(x,y)/(np.linalg.norm(x.toarray())*np.linalg.norm(y.toarray()))\n",
        "        return r\n",
        "    #returns the exact jaccard similarity of 2 vectors\n",
        "    def jaccardSimilarity(x,y):\n",
        "        intersection=len(x.intersection(y))\n",
        "        union=len(x.union(y))\n",
        "        return intersection/union"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this is the hash class used to generate an hash function to be called on an input.\n",
        "generate nBytes vectors, multiply it for the input and the resulting vector will have 1 if the elements are positive or 0 if <= 0.\n",
        "This hash is used in the normal lsh not in the vectorized lsh that i wrote"
      ],
      "metadata": {
        "id": "kQD3Yhng3fB0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QC8HNHma8p3N"
      },
      "outputs": [],
      "source": [
        "class Hash:\n",
        "    def __init__(self,nBytes=50,inputDimension=10):\n",
        "        self.nBytes=nBytes\n",
        "        self.inputDimension=inputDimension\n",
        "        self.reset()\n",
        "    #reset the matrix to generate the hash\n",
        "    def reset(self):\n",
        "        self.matrix=np.random.randn(self.nBytes,self.inputDimension)\n",
        "    #generate the hash as described above\n",
        "    def generateHash(self,input):\n",
        "        if type(input)==list:\n",
        "            input=np.array(input)\n",
        "        bools = (np.dot(input, self.matrix.T) > 0).astype('int')\n",
        "        return ''.join(bools.astype('str'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tz7w65JG8p3O",
        "outputId": "75df8a3b-08aa-4fb7-ab1c-f98f05d732ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'01011010011001011100010001110101101010111111001101'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "h=Hash()\n",
        "h.generateHash(np.ones(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this class is responsible to perform the generation of the m matrix both for the train and for the test datasets , perform the lsh and compare the results for the approximated results and the exact results"
      ],
      "metadata": {
        "id": "duwnbTuX4Nd6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "1X0ld-_b8p3P"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LshNormal:\n",
        "    def __init__(self,X,reviews,b=10,r=10,testPercentage=0.005,nHashes=100):\n",
        "        self.X=X\n",
        "        self.reviews=reviews\n",
        "        self.rows=X.shape[0]\n",
        "        self.cols=X.shape[1]\n",
        "        self.b=b\n",
        "        self.r=r\n",
        "        self.testPercentage=testPercentage\n",
        "        self.nHashes=nHashes\n",
        "        self.splitDataset()\n",
        "\n",
        "    def setBR(self,b,r):\n",
        "        self.b=b\n",
        "        self.r=r\n",
        "    #split the dataset in train and test based on percentage\n",
        "    def splitDataset(self):\n",
        "        indexSplit=int(self.rows*self.testPercentage)\n",
        "        self.testReviews,self.trainReviews=self.reviews[:indexSplit],self.reviews[indexSplit:]\n",
        "        self.testX,self.trainX=self.X[:indexSplit],self.X[indexSplit:]\n",
        "        self.rowsTest,self.colsTest=self.testX.shape\n",
        "        self.rowsTrain,self.colsTrain=self.trainX.shape\n",
        "    #fucntion to generate the matrix of signatures \n",
        "    def generateM(self):\n",
        "        self.mTest=[[]for i in range(self.rowsTest)]\n",
        "        self.mTrain=[[]for i in range(self.rowsTrain)]\n",
        "        for i in range(self.nHashes):\n",
        "            #generate a random vector u\n",
        "            u=Operations.generateU()\n",
        "            for j in range(self.rowsTest):\n",
        "                #get the hash for every test row\n",
        "                h=Operations.h(self.testX[j],u)\n",
        "                self.mTest[j].append(h)\n",
        "            for j in range(self.rowsTrain):\n",
        "                #get the hash for every train row\n",
        "                h=Operations.h(self.trainX[j],u)\n",
        "                self.mTrain[j].append(h)\n",
        "    #function to exectute the lsh algorithm\n",
        "    #starting from the m matrix hash r elements at the time for a total of b times and insert the index in the relative bucket\n",
        "    def lsh(self):\n",
        "        self.dicts=[{} for i in range(self.b)]\n",
        "        #create hash functions\n",
        "        self.hashesUsed=[Hash(inputDimension=self.r) for i in range(self.b)]\n",
        "        #for every b\n",
        "        for i in range(self.b):\n",
        "        #for every row of the training set\n",
        "            for j in range(self.rowsTrain):\n",
        "                #get the r values based on b and hash them \n",
        "                v=self.mTrain[j][i*self.r:(i+1)*self.r]\n",
        "                key=self.hashesUsed[i].generateHash(v)\n",
        "                #add the index to the hashes\n",
        "                if key not in self.dicts[i]:\n",
        "                    self.dicts[i][key]=set()\n",
        "                self.dicts[i][key].add(j)\n",
        "    #Ffunction to find the exact cosine similarity\n",
        "    def findExactCosineSimilars(self,threshold=0.5):\n",
        "        self.exactSimilars={}\n",
        "        #for every test row\n",
        "        for i,v in enumerate(self.testX):\n",
        "          #for every train rows\n",
        "            for j,u in enumerate(self.trainX):\n",
        "              #find exact cosine similarity between the corresponding vectors and if greater than threshold save the result\n",
        "                r=Operations.cosineSimilarity(v,u)\n",
        "                if r>=threshold:\n",
        "                    if i not in self.exactSimilars:\n",
        "                        self.exactSimilars[i]=set()\n",
        "                    self.exactSimilars[i].add(j)\n",
        "        return self.exactSimilars\n",
        "    #find the cosine similars starting from the buckets created by the lsh function\n",
        "    def findCosineSimilarsApproximated(self):\n",
        "        self.approximatedSimilars={}\n",
        "        #for every test row of m\n",
        "        for i,v in enumerate(self.mTest):\n",
        "            #for every hash functions used to do the lsh\n",
        "            for j,hash in enumerate(self.hashesUsed):\n",
        "              #key is the signature of the b th block of r values \n",
        "                key=hash.generateHash(v[j*self.r:(j+1)*self.r])\n",
        "                if key in self.dicts[j]:\n",
        "                  #add add all the elements of the bucket where its been hashed to the set of approximated similars\n",
        "                    for k in self.dicts[j][key]:\n",
        "                        if i not in self.approximatedSimilars:\n",
        "                            self.approximatedSimilars[i]=set()\n",
        "                        self.approximatedSimilars[i].add(k)\n",
        "        return self.approximatedSimilars\n",
        "\n",
        "    def findCosineSimilarsApproximatedSingular(self,i):\n",
        "        similars=set()\n",
        "        #for every hash functions used to do the lsh\n",
        "        for j,hash in enumerate(self.hashesUsed):\n",
        "          #key is the signature of the b th block of r values \n",
        "            key=hash.generateHash(self.mTest[i][j*self.r:(j+1)*self.r])\n",
        "            if key in self.dicts[j]:\n",
        "              #add add all the elements of the bucket where its been hashed to the set of approximated similars\n",
        "                for k in self.dicts[j][key]:\n",
        "                    similars.add(k)\n",
        "        return similars\n",
        "    #function to evaluate the jaccard similarity between the true cosine similars and the appoximated cosine similars\n",
        "    def jaccardAB(self):\n",
        "        return Operations.jaccardSimilarity(self.A,self.B)\n",
        "    def createAB(self):\n",
        "        self.A=set()\n",
        "        self.B=set()\n",
        "        for i in self.exactSimilars:\n",
        "            for j in self.exactSimilars[i]:\n",
        "                self.A.add((i,j))\n",
        "        for i in self.approximatedSimilars:\n",
        "            for j in self.approximatedSimilars[i]:\n",
        "                self.B.add((i,j))\n",
        "    #function i wrote to find the r and b that yields to the best jaccard similarity given the cosine similarity and the number of hashes\n",
        "    def optimizeRB(self,cosineSimilarity=0.2,nHashes=10):\n",
        "      self.findExactCosineSimilars(cosineSimilarity)\n",
        "      self.nHashes=nHashes\n",
        "      maxTaken=0\n",
        "      #generate the matrix m\n",
        "      lsh.generateM()\n",
        "      #automatic search for b and r\n",
        "      maxJaccard=0\n",
        "      maxR=0\n",
        "      maxB=0\n",
        "      #try all the r and b until the squared root of the number of hashes try a b and b a\n",
        "      for i in range(1,int(lsh.nHashes**0.5)):\n",
        "        lsh.r=i\n",
        "        lsh.b=int(lsh.nHashes/i)\n",
        "        lsh.lsh()\n",
        "        b=lsh.findCosineSimilarsApproximated()\n",
        "        jacc=lsh.jaccardAB()\n",
        "        #if global optimum save values\n",
        "        if jacc>maxJaccard:\n",
        "          maxJaccard=jacc\n",
        "          maxR=lsh.r\n",
        "          maxB=lsh.b\n",
        "          #find how many true positives have been retrieved correctely\n",
        "          maxTaken=Operations.jaccardSimilarity(self.A.intersection(self.B),self.A)\n",
        "        lsh.b=i\n",
        "        lsh.r=int(lsh.nHashes/i)\n",
        "        lsh.lsh()\n",
        "        b=lsh.findCosineSimilarsApproximated()\n",
        "        jacc=lsh.jaccardAB()\n",
        "        #if global optimum save values\n",
        "        if jacc>maxJaccard:\n",
        "          maxJaccard=jacc\n",
        "          maxR=lsh.r\n",
        "          maxB=lsh.b\n",
        "          #find how many true positives have been retrieved correctely\n",
        "          maxTaken=Operations.jaccardSimilarity(self.A.intersection(self.B),self.A)\n",
        "\n",
        "      print([maxJaccard,maxR,maxB])\n",
        "      print(maxTaken)\n",
        "      self.r=maxR\n",
        "      self.b=maxB\n",
        "      return [maxJaccard,maxR,maxB]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this is the vectorized lsh class.\n",
        "it generates the matrix m using vectorized operations with numpy and performs the lsh and exact similars functions in a vectorized ways.\n",
        "This significately speed things up because operations are parallelized and optimized in c to speed things up."
      ],
      "metadata": {
        "id": "CfnY7kRW5H-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#every operation of lsh has been vectorize here using numpy for better performances\n",
        "class LshVectorized:\n",
        "\n",
        "    def __init__(self,X,reviews,b=10,r=10,testPercentage=0.005,nHashes=100):\n",
        "        self.X=X\n",
        "        self.reviews=reviews\n",
        "        self.rows=X.shape[0]\n",
        "        self.cols=X.shape[1]\n",
        "        self.b=b\n",
        "        self.r=r\n",
        "        self.testPercentage=testPercentage\n",
        "        self.nHashes=nHashes\n",
        "        self.splitDataset()\n",
        "\n",
        "    def setBR(self,b,r):\n",
        "        self.b=b\n",
        "        self.r=r\n",
        "    #split the dataset in train and test set\n",
        "    def splitDataset(self):\n",
        "        indexSplit=int(self.rows*self.testPercentage)\n",
        "        self.testReviews,self.trainReviews=self.reviews[:indexSplit],self.reviews[indexSplit:]\n",
        "        self.testX,self.trainX=self.X[:indexSplit],self.X[indexSplit:]\n",
        "        self.rowsTest,self.colsTest=self.testX.shape\n",
        "        self.rowsTrain,self.colsTrain=self.trainX.shape\n",
        "    #generate the matrix of the signatures\n",
        "    def generateMMM(self):\n",
        "      #generate random normal distributed vectors U with shape n of cols of the document and number of hashes\n",
        "      allU=np.random.normal(0,1,[self.cols,self.nHashes])\n",
        "      #transformt into sparse matrix to make the next multiplication faster \n",
        "      allU=scipy.sparse.csr_matrix(allU)\n",
        "      #multiply the test and train tfidf vectors with the u matrix and take the sign of the resulting matrix \n",
        "      #the obtained 2 matrices are the matrices M obtained with vectorized operations\n",
        "      array=np.array((self.testX * allU).toarray())\n",
        "      self.mTest=np.sign(array)\n",
        "      array=np.array((self.trainX * allU).toarray())\n",
        "      self.mTrain=np.sign(array)\n",
        "    #lsh done using vectorized operations\n",
        "    def lshMM(self):\n",
        "      #create atrix of random numbers with dimension b, 30 that is the number of vectors for each hash and r\n",
        "      hashes=np.random.randn(self.b,30,self.r)\n",
        "      self.hashesMM=hashes\n",
        "      #take the firsts r*b elements of m and those will compose the train and test signature \n",
        "      signatureTrain=self.mTrain[:,0:self.r*self.b].reshape(-1,self.b,self.r)\n",
        "      #produce a matrix of size number of rows x b(number of hash functions)\n",
        "      #i used the einsum operator and the hash generated with this expression for every element \n",
        "      #is the element multiplied by a matrix of signatures of size row x hashes and taking the sum of the entries of the resulting vector\n",
        "      #in this way in each entry r b i will have a value corresponding to the key of the bucket in which it has to be hashed seeing it in the normal way      \n",
        "      self.signatureTrain=np.einsum(\"abr, bdr-> ab\",signatureTrain,hashes)\n",
        "      #same for the test signatures\n",
        "      signatureTest=self.mTest[:,0:self.r*self.b].reshape(-1,self.b,self.r)\n",
        "      self.signatureTest=np.einsum(\"abr, bdr-> ab\",signatureTest,hashes)\n",
        "    #starting from the matrix of the signatures find the indices of the rows that are equal to the signature this is done in a vectorized way\n",
        "    def findCosineSimilarsApproximatedMM(self):\n",
        "        self.approximatedSimilars={}\n",
        "        #for every signature test\n",
        "        for i,signature in enumerate(self.signatureTest):\n",
        "          #non zero returns rows and columns that satisfy the condition signature train equal to signature(signature is broadcasted on all the rows)\n",
        "          #in this way at every key i i will have the set of row indices that have equal signature in the corresponding bucket(same column)\n",
        "          self.approximatedSimilars[i]=set((self.signatureTrain==signature).nonzero()[0])\n",
        "        return self.approximatedSimilars\n",
        "    #does the same of the previous function but for one single value so it returns the indices of the elements that have the right signature\n",
        "    def findCosineSimilarsApproximatedMMSingular(self,i):\n",
        "        return set((self.signatureTrain==self.signatureTest[i,:]).nonzero()[0])\n",
        "    #this function finds the elements with the exact cosine similarity in a vectorized way\n",
        "    def findExactCosineSimilarsMM(self,threshold=0.5):\n",
        "        self.exactSimilars={}\n",
        "        #i create a matrix of the similaritis multiplisg the test vectors with the trian vectors trasposed and then normalizing with the norms\n",
        "        sim=self.testX.dot(self.trainX.T)\n",
        "        #i take the norms of the vectors of norm and test\n",
        "        normTest=scipy.sparse.linalg.norm(self.testX,axis=1)\n",
        "        normTrain=scipy.sparse.linalg.norm(self.trainX,axis=1)\n",
        "        #i divide sim by the norm of train and for the norm of test transposed and find the indices ( rows and columns) of the elements\n",
        "        # with a selected thresold \n",
        "        sim=sim/normTrain\n",
        "        sim=(sim.T/normTest).T\n",
        "        similars=(sim>=threshold).nonzero()\n",
        "        #i create the dictionary of the similars for every element of the test set\n",
        "        for i in range(len(similars[0])):\n",
        "          if similars[0][i] not in self.exactSimilars:\n",
        "            self.exactSimilars[similars[0][i]]=set()\n",
        "          self.exactSimilars[similars[0][i]].add(similars[1][i])\n",
        "        return self.exactSimilars\n",
        "    # function used to find the optimal values of r and b given cosine similarity and n of hashes\n",
        "    def optimizeRBMM(self,cosineSimilarity=0.2,nHashes=10):\n",
        "      self.findExactCosineSimilarsMM(cosineSimilarity)\n",
        "      self.nHashes=nHashes\n",
        "      maxTaken=0\n",
        "      lsh.generateMMM()\n",
        "      #automatic search for b and r\n",
        "      maxJaccard=0\n",
        "      maxR=0\n",
        "      maxB=0\n",
        "      for i in range(1,int(lsh.nHashes**0.5)):\n",
        "        lsh.r=i\n",
        "        lsh.b=int(lsh.nHashes/i)\n",
        "        lsh.lshMM()\n",
        "        b=lsh.findCosineSimilarsApproximatedMM()\n",
        "        jacc=lsh.jaccardAB()\n",
        "        if jacc>maxJaccard:\n",
        "          maxJaccard=jacc\n",
        "          maxR=lsh.r\n",
        "          maxB=lsh.b\n",
        "          maxTaken=Operations.jaccardSimilarity(self.A.intersection(self.B),self.A)\n",
        "        lsh.b=i\n",
        "        lsh.r=int(lsh.nHashes/i)\n",
        "        lsh.lshMM()\n",
        "        b=lsh.findCosineSimilarsApproximatedMM()\n",
        "        jacc=lsh.jaccardAB()\n",
        "        if jacc>maxJaccard:\n",
        "          maxJaccard=jacc\n",
        "          maxR=lsh.r\n",
        "          maxB=lsh.b\n",
        "          maxTaken=Operations.jaccardSimilarity(self.A.intersection(self.B),self.A)\n",
        "      print([maxJaccard,maxR,maxB])\n",
        "      print(maxTaken)\n",
        "      self.r=maxR\n",
        "      self.b=maxB\n",
        "      return [maxJaccard,maxR,maxB]\n",
        "    #create the sets a and b\n",
        "    def createAB(self):\n",
        "        self.A=set()\n",
        "        self.B=set()\n",
        "        for i in self.exactSimilars:\n",
        "            for j in self.exactSimilars[i]:\n",
        "                self.A.add((i,j))\n",
        "        for i in self.approximatedSimilars:\n",
        "            for j in self.approximatedSimilars[i]:\n",
        "                self.B.add((i,j))\n",
        "    #finds jaccard similarity of exact and approximated similars couples\n",
        "    def jaccardAB(self):\n",
        "        return Operations.jaccardSimilarity(self.A,self.B)\n"
      ],
      "metadata": {
        "id": "oPfjXD8_4hg_"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "start by taking a percentage of the dataset if you want to test the functions on just a portion of the dataset before using everything on the entire dataset.\n",
        "If you use the non vectorized lsh there is the need to take a small percentage of the dataset otherwise it will take too much time because python is slow if it doesnt use parallelized and optimized libraries "
      ],
      "metadata": {
        "id": "5-WuFIYNNLo9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "PTgnEYSp8p3Q"
      },
      "outputs": [],
      "source": [
        "reviews,X=readReviewsX()\n",
        "reviews,X=percentageOfDataset(reviews,X,0.2)\n",
        "rows,cols=X.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "here you can use the vectorized lsh"
      ],
      "metadata": {
        "id": "3i2-LmoA6Dih"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "vZKzD13u8p3Q"
      },
      "outputs": [],
      "source": [
        "lsh=LshVectorized(X,reviews)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "r and b can be selected and used to generate M and to calculate the lsh"
      ],
      "metadata": {
        "id": "9XSTAkFH6Hau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lsh.r=5\n",
        "lsh.b=5\n",
        "lsh.nHashes=lsh.r*lsh.b\n",
        "lsh.generateMMM()\n",
        "lsh.lshMM()"
      ],
      "metadata": {
        "id": "RLNgsALy0yHN"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "passing the index of a test vector we can see all the similars to it in the training set"
      ],
      "metadata": {
        "id": "pisH3pZx6P5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lsh.findCosineSimilarsApproximatedMMSingular(1)\n",
        "\"finished\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2MBzdvCxu5WQ",
        "outputId": "928fac3a-a5e9-4fc1-8117-fbcedc7a78f8"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'finished'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this function calculates the exact similars for the entire test set use it only on a portion of the dataset otherwise the session will crash because it uses a lot of memory but is ultra fast"
      ],
      "metadata": {
        "id": "nZ1sloZj6Yna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lsh.findExactCosineSimilarsMM(0.3)\n",
        "\"finished\""
      ],
      "metadata": {
        "id": "bvh-i0W34FPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "use this function to calculate all the cosine similaras for every vector in the test set use it on a portion of the dataset or when not too many vectors are found otherwise too much memory is used"
      ],
      "metadata": {
        "id": "dQRkjZz46mLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lsh.findCosineSimilarsApproximatedMM()\n",
        "\"finished\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1fxl9mVi4iDS",
        "outputId": "c014dd4c-5f94-47d2-d5eb-a27766fa1be5"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'finished'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this function creates the a and b sets as in the homework description as sets of tuples"
      ],
      "metadata": {
        "id": "kWrqUv7U61qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lsh.createAB()\n",
        "\"finished\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uVRzhtGB4xDI",
        "outputId": "c25849d5-b9bd-4352-f870-99d6e6c4a5e1"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'finished'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "print the dimensions of the 2 sets their differenceBA( how many false positive ), their intersection ( positives correctively found), jaccard and differenceAB(how many false negatives)"
      ],
      "metadata": {
        "id": "WiBlepJb7Eme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"dima={len(lsh.A)}, dimB={len(lsh.B)} , differenceBA={len(lsh.B.difference(lsh.A))} , intersection={len(lsh.B.intersection(lsh.A))} , jaccard={lsh.jaccardAB()} , differenceAB={len(lsh.A.difference(lsh.B))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDYQZL_r66SY",
        "outputId": "168d713a-d200-4905-c3e5-26a2053c21d8"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dima=26009, dimB=10256217 , differenceBA=10246294 , intersection=9923 , jaccard=0.0009659956486875436 , differenceAB=16086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "those two functions have been useful to select b and r and to see if everything was working correctly (analyzing the intersection of a and b / a wrt the function  y=1-(1-x^r)^b  that gives the probability of an object with similarity x being found by lsh"
      ],
      "metadata": {
        "id": "86sSDrTk7lq2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "function that starting from cosine similarity gives what p(x==y) approximates"
      ],
      "metadata": {
        "id": "qYogQ30-5UQR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3kSv85iDQ6-",
        "outputId": "4543bebf-04ef-481f-f4a7-178e08fd6851"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6309898804344547"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fromsim=0.4\n",
        "top=1-np.arccos(fromsim)/np.pi\n",
        "top"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "function that starting from p(x==y) gives the cosine similarity\n",
        "from the funciton"
      ],
      "metadata": {
        "id": "ALBbsL1m5gCp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCvzYEM4DmW0",
        "outputId": "2dbf8c4b-354f-401a-f9aa-59a50e3a8440"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.30901699437494745"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "fromp=0.6\n",
        "tosim=np.cos((1-fromp)*np.pi)\n",
        "tosim"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "05c73f36370b37d7956a45e693a83b97388709ec0b54883dc202775cd3dcb887"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}